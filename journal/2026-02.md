# 2026年2月学习笔记

## 2.03 - 学习 Vector Database

### 背景
对话历史太长会超 token，想着能不能只"检索相关"的历史，而不是全部带上。

### 第一次尝试：自己实现关键词搜索

```python
class SimpleMemory:
    def search(self, query):
        results = []
        for msg in self.history:
            if keyword in msg:
                results.append(msg)
        return results
```

**问题：** 只能精确匹配，"我喜欢猫"和"猫很可爱"算相关吗？不算

### 第二次尝试：学习 TF-IDF

**理论：** 把文本转成向量，计算相似度

**问题：**
- 理论看不懂（词向量、余弦相似度...）
- 算出来一堆数字，不知道怎么解释

### 第三次尝试：用 ChromaDB

### 安装
```bash
pip install chromadb
```

### 尝试代码

```python
import chromadb

client = chromadb.PersistentClient(path="./chroma_db")
collection = client.get_or_create_collection("chat_history")

# 添加文档
collection.add(
    documents=["用户说：我叫小王", "用户说：我喜欢编程"],
    ids=["msg1", "msg2"]
)

# 查询
results = collection.query(
    query_texts=["我叫什么名字"],
    n_results=2
)
```

### 效果
✅ **能用了！**
- 查询"我叫什么名字"，正确返回了"用户说：我叫小王"

### 问题
1. **Embedding 怎么做？**
   - ChromaDB 似乎自动用了默认模型（sentence-transformers）
   - 但性能怎么样？要不要换成 OpenAI 的 embedding？

2. **检索效果**
   - 有时返回不相关的内容
   - 比如查询"今天天气"，却返回了昨天的对话
   - 需要调 `n_results` 或过滤？

### 代码位置
见 `v3/memory/simple_vector_db.py`

---

## 2.15 - 尝试 OpenAI Embedding

### 背景
ChromaDB 默认的 embedding 模型不知道怎么样，决定用 OpenAI 的试试。

### 尝试代码

```python
from openai import OpenAI
import chromadb

openai_client = OpenAI()

# 先用 OpenAI 生成 embedding
text = "用户说：我叫小王"
response = openai_client.embeddings.create(
    model="text-embedding-3-small",
    input=text
)
embedding = response.data[0].embedding

# 再存入 ChromaDB
collection.add(
    documents=[text],
    embeddings=[embedding],
    ids=["msg1"]
)
```

### 效果
✅ **检索准确很多！**
- "我叫什么名字"几乎只返回相关的对话
- 相关性评分更合理

### 问题
1. **成本**
   - 每次对话前，要调用 embedding API（$0.00002/1K tokens）
   - 用户聊 100 句话，就要花 2 美分
   - 长期下来不划算？

2. **延迟**
   - 每次查询前要等 OpenAI embedding
   - 响应时间增加了 0.5-1 秒

### 结论
- **质量**：OpenAI embedding 准确很多
- **成本**：太高，不适合频繁调用的场景
- **备选**：考虑本地模型（如 sentence-transformers），虽然质量可能稍差，但免费

---

## 2.25 - 学习 RAG（检索增强生成）

### 背景
现在有：
1. 对话历史的向量检索
2. OpenAI 的 Function Calling

想结合起来：Agent 回答时，先检索相关历史，再调用工具。

### RAG 流程

```
1. 用户提问
2. 向量检索相关历史 → 得到 3-5 条对话
3. 组装提示词：
   "相关对话：{检索到的内容}
   当前问题：{用户问题}"
4. 调用 LLM
5. LLM 决定是否需要工具
6. 执行工具
7. 总结结果
```

### 第一次实现

**代码位置：** `v3/rag/simple_rag.py`

**问题：**
- 提示词太长，token 直接爆炸
- 检索到 5 条历史，每条 200 字，就是 1000 字了

### 第二次实现：只检索最新 1 条

**效果：**
- Token 降下来了
- 但信息不足，有时回答不对

### 结论
- RAG 的核心不是"多检索"，而是"精准检索"
- 需要更好的检索策略，不是简单的前 N 条
- **当前卡点：** 怎么平衡"信息充足"和"token 限制"？

---

## 本月小结

✅ **学会的：**
- Vector Database 的基本概念和使用
- OpenAI Embedding 的调用
- RAG 的基本流程

❌ **困惑的：**
- 成本怎么控制？（embedding 太贵）
- 检索策略怎么优化？
- 多步骤任务怎么编排？（查资料 + 计算 + 生成）

📝 **下一步：**
- 学习 LangChain 的 Memory 模块（看别人怎么解决这些问题）
- 尝试本地 embedding 模型
- 实现一个完整的小项目（哪怕简陋）

---

## 疑问（如果有懂的朋友，欢迎指点）

1. **ChromaDB vs Pinecone vs Qdrant？**
   - 我在用 ChromaDB（本地），但感觉性能一般
   - Pinecone 需要付费吗？
   - Qdrant 好吗？

2. **Token 计算的准确率？**
   - 用 tiktoken 算的，和实际 billed tokens 有 5-10% 的差异
   - 是我算错了吗？还是 billing 本身有误差？

3. **多轮对话的最佳实践？**
   - 是不是每次都检索？
   - 还是最开始检索一次，后面都用同一份上下文？
